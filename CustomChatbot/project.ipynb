{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "124e5672",
   "metadata": {},
   "source": [
    "# Custom Chatbot Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4a94b3",
   "metadata": {},
   "source": [
    "TODO: In this cell, write an explanation of which dataset you have chosen and why it is appropriate for this task\n",
    "\n",
    "For this exercise, I choose the Fashion Trends for 2023 dataset. Using the Fashion Trends for 2023 dataset ensures the chatbot is relevant, engaging, and valuable for both businesses and users. The combination of retrieval capabilities and generative AI allows the chatbot to offer both precision and creativity, making it an essential tool in the fashion domain.\n",
    "\n",
    "Below are some potential applications:\n",
    "\n",
    "1. Personalized Styling Recommendations\n",
    "Virtual Stylist: Suggests outfits based on user preferences, body type, occasion, or current fashion trends.\n",
    "Wardrobe Planning: Helps users mix and match clothing they own with trending styles.\n",
    "Event-Specific Advice: Provides recommendations for events like weddings, parties, or job interviews.\n",
    "\n",
    "2. E-Commerce Support\n",
    "Product Discovery: Assists customers in finding products that match trending styles (e.g., \"Show me dresses in 2023's trending colors\").\n",
    "Upselling & Cross-Selling: Recommends complementary items to enhance an outfit (e.g., \"Pair this jacket with these accessories\").\n",
    "Trend Updates: Educates users about new arrivals that align with 2023 trends.\n",
    "\n",
    "3. Fashion Education and Awareness\n",
    "Trend Insights: Explains why specific styles, colors, or patterns are trending in 2023.\n",
    "Sustainability Advice: Guides users on eco-friendly fashion options based on the dataset.\n",
    "Cultural Fashion Trends: Highlights how global cultural influences shape trends.\n",
    "\n",
    "4. Fashion Blogging and Influencer Support\n",
    "Content Creation: Provides inspiration for blog posts or social media captions related to fashion trends.\n",
    "Hashtag Suggestions: Recommends popular hashtags or buzzwords associated with 2023 fashion trends for social media posts.\n",
    "Trend Forecasting: Offers insights into future trend predictions based on current data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63d4c5f",
   "metadata": {},
   "source": [
    "## Data Wrangling\n",
    "\n",
    "TODO: In the cells below, load your chosen dataset into a `pandas` dataframe with a column named `\"text\"`. This column should contain all of your text data, separated into at least 20 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69b83a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a595980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataframe\n",
    "df = pd.read_csv(\"data/2023_fashion_trends.csv\")\n",
    "\n",
    "# check first rows\n",
    "df.head()\n",
    "\n",
    "# get rid of url and source\n",
    "df.drop([\"URL\", \"Source\"], axis=1, inplace=True)\n",
    "\n",
    "# replace trends with text\n",
    "df = df.rename(columns={\"Trends\": \"text\"})\n",
    "\n",
    "# check first rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb3a9fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae769871",
   "metadata": {},
   "source": [
    "## Custom Query Completion\n",
    "\n",
    "TODO: In the cells below, compose a custom query using your chosen dataset and retrieve results from an OpenAI `Completion` model. You may copy and paste any useful code from the course materials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582f0656",
   "metadata": {},
   "outputs": [],
   "source": [


   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6e1f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up variables for model and max tokens\n",
    "COMPLETION_MODEL_NAME = \"gpt-3.5-turbo-instruct\"\n",
    "MAX_TOKENS = 2400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f2dcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare embeddings\n",
    "EMBEDDING_MODEL_NAME = \"text-embedding-ada-002\"\n",
    "batch_size = 25\n",
    "embeddings = []\n",
    "for i in range(0, len(df), batch_size):\n",
    "    # Send text data to OpenAI model to get embeddings\n",
    "    response = openai.Embedding.create(\n",
    "        input=df.iloc[i:i+batch_size][\"text\"].tolist(),\n",
    "        engine=EMBEDDING_MODEL_NAME\n",
    "    )\n",
    "    \n",
    "    # Add embeddings to list\n",
    "    embeddings.extend([data[\"embedding\"] for data in response[\"data\"]])\n",
    "\n",
    "\n",
    "\n",
    "# Add embeddings list to dataframe\n",
    "df[\"embeddings\"] = embeddings\n",
    "# check cotent and embeddigins\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c403f543",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create simple prompt\n",
    "def create_simple_prompt(question):\n",
    "   \n",
    "    prompt_template = \"\"\"\n",
    "Question: {}\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    return prompt_template.format(\"\\n\\n###\\n\\n\", question)\n",
    "    \n",
    "# define how to answer to a prompt\n",
    "def answer_prompt(\n",
    "    prompt, df, max_answer_tokens=150\n",
    "):\n",
    "   \n",
    "    \n",
    "    try:\n",
    "        response = openai.Completion.create(\n",
    "            model=COMPLETION_MODEL_NAME,\n",
    "            prompt=prompt,\n",
    "            max_tokens=max_answer_tokens\n",
    "        )\n",
    "        return response[\"choices\"][0][\"text\"].strip()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74280b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.embeddings_utils import get_embedding, distances_from_embeddings\n",
    "\n",
    "def get_rows_sorted_by_relevance(question, df):\n",
    "    \"\"\"\n",
    "    Function that takes in a question string and a dataframe containing\n",
    "    rows of text and associated embeddings, and returns that dataframe\n",
    "    sorted from least to most relevant for that question\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get embeddings for the question text\n",
    "    question_embeddings = get_embedding(question, engine=EMBEDDING_MODEL_NAME)\n",
    "    \n",
    "    # Make a copy of the dataframe and add a \"distances\" column containing\n",
    "    # the cosine distances between each row's embeddings and the\n",
    "    # embeddings of the question\n",
    "    df_copy = df.copy()\n",
    "    df_copy[\"distances\"] = distances_from_embeddings(\n",
    "        question_embeddings,\n",
    "        df_copy[\"embeddings\"].values,\n",
    "        distance_metric=\"cosine\"\n",
    "    )\n",
    "    \n",
    "    # Sort the copied dataframe by the distances and return it\n",
    "    # (shorter distance = more relevant so we sort in ascending order)\n",
    "    df_copy.sort_values(\"distances\", ascending=True, inplace=True)\n",
    "    return df_copy\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "def create_RAG_prompt(question, df, max_token_count):\n",
    "    \"\"\"\n",
    "    Given a question and a dataframe containing rows of text and their\n",
    "    embeddings, return a text prompt to send to a Completion model\n",
    "    \"\"\"\n",
    "    # Create a tokenizer that is designed to align with our embeddings\n",
    "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    \n",
    "    # Count the number of tokens in the prompt template and question\n",
    "    prompt_template = \"\"\"\n",
    "Answer the question based on the context below, and if the question\n",
    "can't be answered based on the context, say \"I don't know\"\n",
    "\n",
    "Context: \n",
    "\n",
    "{}\n",
    "\n",
    "---\n",
    "\n",
    "Question: {}\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    current_token_count = len(tokenizer.encode(prompt_template)) + \\\n",
    "                            len(tokenizer.encode(question))\n",
    "    \n",
    "    context = []\n",
    "    for text in get_rows_sorted_by_relevance(question, df)[\"text\"].values:\n",
    "        \n",
    "        # Increase the counter based on the number of tokens in this row\n",
    "        text_token_count = len(tokenizer.encode(text))\n",
    "        current_token_count += text_token_count\n",
    "        \n",
    "        # Add the row of text to the list if we haven't exceeded the max\n",
    "        if current_token_count <= max_token_count:\n",
    "            context.append(text)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return prompt_template.format(\"\\n\\n###\\n\\n\".join(context), question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1783f146",
   "metadata": {},
   "source": [
    "## Custom Performance Demonstration\n",
    "\n",
    "TODO: In the cells below, demonstrate the performance of your custom query using at least 2 questions. For each question, show the answer from a basic `Completion` model query as well as the answer from your custom query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f11fdc0",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4901c850",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = \"What are the top 5 fashion trends of 2023??\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7a093b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Print answer without RAG\n",
    "print('\\nQ1: Answer without RAG: \\n', answer_prompt(create_simple_prompt(q1), df))\n",
    "print('\\nQ1: Answer with RAG: \\n', answer_prompt(create_RAG_prompt(q1, df, 2000), df))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e86e37c",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f646989",
   "metadata": {},
   "outputs": [],
   "source": [
    "q2 = \"What colors are in style this spring 2023?\"\n",
    "\n",
    "print('\\nQ2: Answer without RAG: \\n', answer_prompt(create_simple_prompt(q2), df))\n",
    "print('\\nQ2: Answer with RAG: \\n', answer_prompt(create_RAG_prompt(q2, df, 2000), df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c07a54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8a6095",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) y",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
